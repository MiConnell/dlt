---
slug: dlt-mds
title: "The Pythonic MDS: Embracing EL as Code"
image:  https://storage.googleapis.com/dlt-blog-images/pymds.png
authors:
  name: Adrian Brudaru
  title: Open source Data Engineer
  url: https://github.com/adrianbr
  image_url: https://avatars.githubusercontent.com/u/5762770?v=4
tags: [MDS, modern data stack, dlt]
---

# The Evolution of the Modern Data Stack

Before the Pythonic modern data stack, we had Saas vendors and custom python scripts, and it sucked. Saas was limited and expensive. Custom scripts broke ALL THE TIME.

The Pythonic Modern Data Stack (MDS) changes how organizations manage and use data.
It focuses on "EL as Code" methods, which improve flexibility, scalability,
and efficiency in data handling. This article explains the importance of EL as
Code in MDS, its advantages, how it works with data orchestration platforms, and its
effect on data engineering.




## Your orchestrator probably runs python

Python is now key for data orchestration.
This is due to tools being designed around user needs and requirements.
Data teams today do more than just reports. They handle complex data workflows including Extract,
Load, and Transform (ELT) and automate tasks like lead scoring. Python-friendly orchestrators
are vital. They provide features for managing workflows, controlling processes, and
checking operations, meeting the demands of modern data tasks.

Python-based orchestrators are popular for their versatility.
They handle a variety of data tasks, from basic to complex, and
support data science and automation. Tools like Apache Airflow, Prefect,
Dagster, and Kestra offer Python-compatible environments. These platforms are good for
teams wanting to use Python for data workflows.

With Python leading in data orchestration, there's a move towards EL as
Code. This method makes data extraction and loading tasks programmable. It
brings the best practices from software development to data engineering. This
improves scalability, repeatability, and quality.


### Managed is nice

Opting for a managed orchestrator simplifies your workflow. Services like Amazon MWAA,
Astronomer, Google Cloud Composer Airflow, Dagster, Modal, and Kestra handle the complex parts of
data orchestration. This allows you to focus on designing data pipelines with Python.
Managed means less time on setup and maintenance, and more on productivity.
Itâ€™s a smart choice for teams looking to boost efficiency and reliability in their
data operations, without the overhead of managing infrastructure.

## EL as Code: From Code to SaaS to Code again

### The Java age: By software devs, for data peeps
In the early stages of data management, ETL tasks depended heavily on bespoke coding.
High-performance languages and platforms such as C and Java were pivotal.
They offered the speed and efficiency required for data handling, surpassing what databases
like MySQL or PostgreSQL could achieve. This period presented significant hurdles,
especially for Business Intelligence (BI) managers and professionals who were reluctant
to immerse themselves in complex coding to construct data pipelines.

With the advancement of digital technologies, the variety and sources of data expanded.
This development introduced tools offering new Application Programming Interfaces (APIs),
increasing the demand for ETL pipelines that could seamlessly integrate these APIs.
The complexity of creating custom pipelines and the steep learning curve associated with
mastering Java or similar languages made this task daunting.

This scenario led to a growing interest in more user-friendly ETL solutions.
Tools such as Pentaho and Talend, which provided graphical interfaces and simpler
integration methods, gained popularity. These platforms made it easier to connect to
various data sources through connectors like JDBC (Java Database Connectivity),
simplifying the ETL process.

![Java Pipeline](https://storage.googleapis.com/dlt-blog-images/java_pipeline.gif)

The shift towards accessible ETL solutions marked a pivotal change, significantly broadening
the accessibility of data pipeline creation. Tools like 5tran and other SaaS-based ETL
platforms democratized data integration, offering easy-to-use interfaces and pre-built connectors.
These solutions excelled in making data operations more efficient and user-friendly,
allowing for quick adaptation to new data sources without requiring in-depth coding skills.
However, while these tools streamlined the ETL process, they often came with limitations in
customization and control. Organizations found that the one-size-fits-all approach of SaaS ETL
tools couldn't always meet the nuanced demands of complex data workflows, leading to compromises
in how data was managed and integrated.

### Gaining an edge with custom scripts
Simply adopting the same SaaS pipelines as competitors won't give you a competitive advantage.
While SaaS ETL tools offer convenience, they often fall short in handling complex data operations.
This limitation has led to a hybrid approach, combining the use of SaaS solutions with custom Python
ETL scripts. This strategy allows for routine data tasks to leverage the efficiency of SaaS tools,
while the more demanding aspects of data engineering, which require extensive customization and deep
integration, are addressed through ETL as code.

This blend of solutions ensures that businesses can maintain the agility and simplicity of
SaaS for straightforward tasks, yet still harness the full power of custom coding for nuanced,
intricate data operations. It's in these custom, often complex, solutions where businesses can
truly differentiate themselves and gain a competitive edge.

As the need for more tailored solutions grew, organizations started integrating custom
Python scripts with their ETL processes. Python scripts offered the flexibility and control
that SaaS tools lacked, enabling more complex data operations and filling the gaps in SaaS
platforms' capabilities. Yet, this shift towards custom scripts also introduced its own set of
challenges. The reliance on bespoke solutions for each new data problem led to issues with maintainability,
efficiency, and scalability. Continuously building from scratch was at odds with the core
principles of software engineering, which emphasizes reusability and efficiency across projects.

This backdrop set the stage for the adoption of "EL as Code" as a unified, Python-centric
approach in data engineering. "EL as Code" treats data extraction and loading tasks as
programmable code, harnessing Python's adaptability and robustness. This method
revolutionized data operations by ensuring better scalability, repeatability, and
quality control. Incorporating the best practices of software development into data
engineering, "EL as Code" provides a structured and more efficient framework for managing
data pipelines, becoming an essential strategy for supporting data-driven decision-making and analytics.

### EL as code brings software engineering practices to the data engineering ~~scene~~ shitshow

![Python Modern Data Stack](https://storage.googleapis.com/dlt-blog-images/pymds.png)

Python's role in data orchestration has revolutionized the modern data stack (MDS).
Its comprehensive libraries, supportive community, and inherent flexibility
have made data management more accessible, reliable, and scalable.
Teams now leverage Python to streamline workflows, automate processes, and tackle
complex challenges efficiently.

This change heralded the era of the Python-first data engineer and highlighted a critical
shift with the introduction of the "EL as Code" approach. This methodology signifies a fusion
of data engineering with software engineering best practices, addressing several key areas:

- Flexibility and Customizability: It allows for easy adjustments as project needs evolve.
- Collaboration through Version Control: Enables effective teamwork and streamlined change management.
- Automation and Scalability: Facilitates scalable operations through orchestrated Python scripts.
- Simplified Stack: Offers straightforward integration with orchestrators, easing the initiation for data engineers.

Historically, the gap between how data engineers and software engineers approached problem-solving
could lead to inefficiencies. The practices employed in data engineering, before the advent of EL as
Code, might have been perplexing to software engineers. Observing data engineers manually managing
processes that could be automated, or struggling with scalability issues due to non-modular code,
could have made a software engineer cry. They would see a significant opportunity for improvement,
advocating for principles like DRY (Don't Repeat Yourself) and modularization, which are staples in
software engineering.

The revival of ETL as code, under the broader movement towards Infrastructure as Code (IaC),
moves away from manual resource management to automated, code-managed processes. By applying
"EL as Code," data engineering adopts a software development mindset, crafting data infrastructures
that are not only more robust and scalable but also foster a collaborative and iterative development
environment. This shift not only bridges the gap between data and software engineering practices but
also raises the bar for data management and operational efficiency.


## The Transform tools are all about better software engineering

The transformation stage is pivotal in turning raw data into actionable insights, standing as a
crucial phase in data engineering. The landscape of data transformation tools has evolved significantly,
offering a variety of solutions tailored to different needs and complexities. Leading this evolution
is dbt (data build tool), renowned for simplifying the conversion of raw data into analytical insights
through SQL.

### dbt: A standard in data transformation, towards better software
Dbt revolutionized data transformation by integrating software engineering best practices into
SQL workflows. It made version control essential, promoting collaboration and enhancing data integrity.
For instance, dbt's use of Git for tracking changes and enabling code reviews ensures that data
transformations are reliable and maintainable. Additionally, dbt supports continuous integration
and deployment (CI/CD), automating the testing and deployment of SQL code changes.
This approach not only streamlines data operations but also aligns SQL practices with the disciplined
methodologies of software development, significantly improving the quality of data projects.

### Other tools, same problem, different solutions
Alternative tools like [SQLMesh](https://sqlmesh.com/) and [SQLglot](https://github.com/tobymao/sqlglot) demonstrate the diverse capabilities available for data engineering. SQLMesh excels in DataOps by offering robust data transformation, testing, and collaboration tools, including real-time SQL validation and semantic understanding. SQLglot enhances SQL's flexibility, aiding in the parsing and generation of code across multiple dialects. Together, these tools guarantee not just the accuracy and efficiency of data pipelines but also the adaptability of SQL code across different database technologies. For in-depth information, their websites and GitHub pages are invaluable resources.

[Sdf](https://www.sdf.com/), a sophisticated compiler and build system, leverages static analysis to examine SQL code thoroughly for issues such as security vulnerabilities and privacy breaches, assessing all SQL queries in unison. Its standout feature, Code Checks, promotes org-scale data privacy, quality, and governance, making it indispensable for safe and reliable data development practices.

[Yato](https://github.com/Bl3f/yato) focuses on enhancing data processing and manipulation, streamlining complex data operations. Its GitHub repository offers a wealth of information on its capabilities and applications.

### Choosing the right Ttol: Aligning with project needs and team expertise

The selection of an appropriate data transformation tool should align with your project's
specific requirements and your team's expertise. Factors such as the complexity of your data
workflows, scalability needs, and technical skills are crucial in this decision-making process.
Additionally, a tool's integration capabilities with your current data stack and its unique
features, like real-time SQL validation or comprehensive data governance, are significant considerations.

Encouraging a culture of continuous learning and experimentation within data teams can
lead to more informed tool selection and enhanced data operations. Keeping abreast of the
latest advancements in data transformation technologies and sharing insights and experiences
within the team can optimize the process of transforming raw data into valuable insights. This not
only ensures the full utilization of chosen tools but also drives innovation and maintains a competitive
edge in the fast-evolving field of data engineering.

## Code-based access layers

Before developing dlt as it stands today, we experimented with a more intricate setup,
crafting an end-to-end analytics pipeline for a Rasa chatbot. During this phase, we devised a
makeshift tool for importing and exporting Metabase configurations, enabling the deployment of
analytical templates to Metabase users through an API.

The concept of "analytics as code," as seen in tools like Holistics,
represents a paradigm shift towards managing analytics workflows and data transformations via code.
This method enhances flexibility and scalability in analytics by embedding the analytical process
within the software development lifecycle. It promotes better team collaboration with version control
and simplifies the management of reports and data lineage.

The introduction of the dbt Semantic Layer, powered by MetricFlow, has transformed
the approach to defining and accessing critical business metrics. By centralizing metric definitions,
it eliminates duplication and guarantees consistent access to metrics across the board. MetricFlow
simplifies the creation and management of these metrics, enabling quick SQL query generation for
efficient metric data retrieval. This advancement empowers users to fully leverage the dbt Semantic Layer,
guiding them through the creation of semantic models, metric definition, testing, querying,
and executing production jobs in dbt Cloud. It's noted that a dbt Cloud Team or Enterprise
account is necessary to unlock the complete range of functionalities.




## The Future: "As code" data engineering with AI collaboration
Advancements in AI and machine learning are shaping the future of data engineering,
focusing on code-driven processes. Leveraging everything-as-code makes data workflows
accessible to large language models (LLMs) and AI, facilitating automation and insight
generation. This approach enhances data quality and trend prediction. The key is integrating
AI into data engineering through customizable Python scripts, moving away from opaque,
black-box solutions.

Collaborating with AI and maintaining code transparency ensures adaptability and paves the way for
more innovative, efficient data management strategies.


## MDS examples with dlt

* [The future now: generated chargebee pipeline](https://dlthub.com/docs/blog/openapi-generation-chargebee)
* [Web tracking on GCP at 18x cost saving over Segment](https://dlthub.com/docs/blog/dlt-segment-migration)
* [Motherduck](https://dlthub.com/docs/blog/dlt-motherduck-demo),
* [google cloud functions ](https://docs.getdbt.com/blog/serverless-dlt-dbt-stack)
* [Gcp streaming cheaper version](https://dlthub.com/docs/blog/streaming-pubsub-json-gcp)
* [Aws streaming](https://dlthub.com/docs/blog/dlt-aws-taktile-blog)

### Want to discuss?

[Join our slack community](https://dlthub.com/community) to take part in the conversation.