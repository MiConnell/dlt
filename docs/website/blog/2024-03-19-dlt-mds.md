---
slug: dlt-mds
title: "The Pythonic MDS: Embracing EL as Code"
image:  https://storage.googleapis.com/dlt-blog-images/pymds.png
authors:
  name: Adrian Brudaru
  title: Open source Data Engineer
  url: https://github.com/adrianbr
  image_url: https://avatars.githubusercontent.com/u/5762770?v=4
tags: [MDS, modern data stack, dlt]
---

# The Evolution of the Modern Data Stack

Before the EVOLVED modern data stack, we had Saas vendors and custom python scripts, and it sucked.

Saas was limited and expensive. Custom scripts broke ALL THE TIME.

The Pythonic Modern Data Stack (MDS) has emerged as a pivotal framework,
revolutionizing how organizations handle, process, and leverage their data.
At the core of this transformation is the adoption of "EL as Code" methodologies,
which prioritize flexibility, scalability, and efficiency in data extraction and loading processes.
This article delves into the significance of EL as Code within the MDS,
highlighting its benefits, integration with data orchestration platforms,
and its profound impact on the data engineering landscape.


## Your orchestrator probably runs python

Python has emerged as the standard in data orchestration, largely because the tools developed for orchestration have been built
around what people use and need. This isn't surprising when you consider the requirements of most data teams
today. Beyond basic reporting tasks, there's a growing need to manage complex data workflows
involving Extract, Load, and Transform (ELT) processes, as well as to automate business operations like
lead scoring and annotating datasets. For these reasons, orchestrators that support Python
have become essential, offering the capabilities such as runtime management, workflow control,
and observability that modern data operations demand.

Given these needs, it's clear why Python-based orchestrators have taken center stage.
They cater to a wide range of data tasks from simple to complex, supporting not just data movement but
also the intricate processes of data science and automation workflows. This is where tools like Apache
Airflow, Prefect, Dagster, and Kestra come into play. Each of these platforms provides a Python-friendly
environment for defining and managing data workflows, making them excellent choices for teams looking to leverage Python’s capabilities.

In the context of Python's dominance in data orchestration, we're also seeing a
shift towards EL as Code, a practice that treats data extraction and loading tasks as
programmable code. This approach brings software development's best practices into data engineering,
offering improved scalability, repeatability, and quality control.


### Managed is nice
So no matter what you like, you have many options. For starters,
orchestrators that are managed will be easier to work with: such as google cloud composer airflow,
dagster, kestra, modal, etc. so pick what fits your needs and likes.
Using a managed orchestrator allows teams to focus on defining their data pipelines in Python,
knowing that the execution environment is robust and can handle tasks efficiently. The choice between these tools often comes down to specific project requirements and personal preference, but the underlying principle is the same: leveraging Python to make EL and T operations as efficient, reliable, and transparent as possible.

This sets the stage for a more in-depth exploration of EL as Code and how it revolutionizes data orchestration, a topic we'll delve into further.


## EL as Code: From Code to SaaS and Back Again

Initially, ETL tasks were heavily reliant on custom coding, with tools and languages like Talend and
Java offering the speed and performance necessary to manage data effectively—far
outpacing the capabilities of databases like MySQL or PostgreSQL at the time.
This time brought challenges especially for BI managers and professionals who were not
inclined to dive into complex coding to build data pipelines.

As the digital landscape evolved, so did the sources of data, as new tools offered new APIs.
This led to an increased demand for ETL pipelines capable of integrating with the APIs.
However, the steep learning curve associated with Java and the complexity of
custom pipeline creation prompted a shift towards more accessible solutions.

Enter the age of SaaS ETL tools, designed to democratize data pipeline construction
without the need for deep programming knowledge. These platforms provided BI managers
with user-friendly interfaces and pre-built connectors, simplifying the ETL process
but at the cost of the customization and control that coding from scratch offered.

The transition from relying solely on SaaS vendors and custom python scripts for data
management revealed significant challenges. SaaS platforms, while user-friendly,
often lacked the flexibility and scalability required for complex data workflows,
imposing limitations on customization and integration. Conversely, custom scripts,
despite their adaptability, were prone to frequent breakdowns and maintenance headaches,
making them a less reliable solution for critical data operations.

The concept of "EL as Code" represents a significant advancement in data engineering,
treating data extraction and loading tasks as programmable code.
This approach has transformed data operations, enabling better scalability,
repeatability, and quality control. By integrating software development best
practices into data engineering, EL as Code offers a more structured and efficient
framework for managing data pipelines, underscoring its strategic importance in enabling data-driven
decision-making and analytics.

### We are not lost! Python leads the way.

![Python Modern Data Stack](https://storage.googleapis.com/dlt-blog-images/pymds.png)

The adoption of Python as the standard in data orchestration has fundamentally altered the
landscape of modern data stack (MDS). Python's emergence is attributed to its extensive
libraries, supportive community, and inherent flexibility, which collectively facilitate a
more accessible, reliable, and scalable approach to data management. This shift has enabled data
teams to leverage Python's capabilities to streamline their workflows, enhance automation, and
tackle complex data challenges more effectively.

The landscape took another turn with the rise of the
Python-first data engineer.

EL as code approach brings several key advantages:

- Flexibility and customisability: custom code that's customisable can be easily adjusted to evolving needs.
- Collaboration though version control: The ability to version-control EL scripts fosters better teamwork and change management.
- Automation and scalability: Python scripts can be orchestrated in many ways to help them scale.
- Simple stack lowers barriers: Native integration in orchestrators enables an easy starting point.

This revival of ETL as code is part of a broader movement towards Infrastructure as Code (IaC) in tech,
moving away from manual resource management to automated, code-managed processes.

### Competitive data usage

To surpass the competition, using SaaS pipelines that competitors use is not enough.

Despite the convenience of SaaS ETL tools, they often lack the depth and flexibility
required for complex data operations, leading to a hybrid approach where custom Python
ETL scripts coexist with SaaS solutions. This strategy ensures that while routine
data tasks can benefit from the efficiency of SaaS tools, the heavy lifting of data
engineering—requiring deep customization and integration—remains firmly in the domain of ETL as code.


## T of your choice, dbt is a standard
The transformation stage in the data pipeline is where raw data is turned into actionable insights,
making it a crucial part of the data engineering process. Over the years, the tools and technologies
available for data transformation have evolved, offering a range of options
tailored to various needs and complexities. At the forefront of this evolution is dbt (data build tool),
a tool that has set the standard for the transformation process by simplifying the conversion of raw
data into analytical insights using SQL.

### dbt: Setting the Standard in Data Transformation

dbt has become synonymous with modern data transformation, primarily due to its ease of use, efficiency,
and the collaborative environment it fosters among data teams. By treating transformations as code,
dbt enables version control, testing, and documentation, which are pivotal for maintaining the
integrity and reliability of data pipelines. However, while dbt offers significant advantages,
it's not without its limitations. For instance, some data engineering tasks may require specialized
functionalities or higher performance than dbt can provide, prompting the exploration of alternative tools.

### Some noteworthy tools in the space

We often hear about dbt or google dataform but here are some tools that are cool for different reasons.

[SQLMesh](https://sqlmesh.com/) and [SQLglot](https://github.com/tobymao/sqlglot), when considered together, offer a powerful combination for data engineering tasks.
SQLMesh provides a comprehensive DataOps solution, emphasizing data transformation,
testing, and collaboration, with features like real-time SQL validation and semantic understanding.
SQLglot, on the other hand, focuses on the flexibility of SQL code, enabling parsing and
generation across multiple dialects. This combination ensures not only that data pipelines
are accurate and efficient but also that SQL code is versatile and compatible with various
database technologies. For specific details on each tool, visiting their respective websites
or GitHub pages is recommended.

[Sdf](https://www.sdf.com/) is a sophisticated compiler and build system that utilizes static analysis to
thoroughly review SQL code across an entire data warehouse. It's designed to help data
developers identify code issues like security vulnerabilities and privacy leaks,
evaluating all SQL queries simultaneously. SDF stands out for its capability
to enforce org-scale data privacy, quality, and governance with its Code Checks feature,
facilitating safer and more reliable data development practices.

[Yato](https://github.com/Bl3f/yato/blob/main/README.md) is a tool designed to offer capabilities in the realm of
data processing and manipulation. It focuses on enhancing the efficiency and
flexibility of handling data tasks, making it easier for users to work with complex
data sets and perform various operations. For a deeper understanding of Yato and its
specific features, visiting the GitHub repository and exploring its documentation would
provide the most comprehensive insight.


### dlt for etl as code

dlt, stands as a modern tool designed to enhance the Extract, Load, Transform (ELT) process within the data engineering landscape. It leverages Python to offer a flexible, code-driven approach to data processing, aligning perfectly with the EL as Code methodology. By incorporating DLT into their workflows, data engineers can efficiently automate and manage data pipelines, ensuring that data extraction, loading, and transformation are executed seamlessly. This integration of DLT within the EL as Code framework significantly improves data operation efficiency, reliability, and scalability. For more details, you can visit DLTHub.

## Access layers with programmatic access
Before we built dlt the way it is now, we played with a more complex version of it creating a rasa chatbot end to end analytics pipeline.
During this experiment, we created a hacky import-export tool for [metabase](https://www.metabase.com/) configuration and pushed analytical templates to new user deployments of Metabase via the api.

Other tools, such as Holistics have approaches of ["analytics as code"](https://docs.holistics.io/as-code/) which means
it allows users to manage
their analytics workflows and data transformations directly through code.
This approach provides a more flexible and scalable way to handle analytics,
as it integrates the analytical process more deeply into the software development
lifecycle. This enables better collaboration among teams through version control, and easier management of
reporting and lineage.

The [dbt Semantic Layer](https://docs.getdbt.com/docs/use-dbt-semantic-layer/dbt-sl), enhanced by MetricFlow, revolutionizes the way critical business metrics
are defined and utilized, centralizing metric definitions to eliminate redundancy and
ensure consistent metric access. MetricFlow facilitates easy creation and management of metrics,
offering SQL query generation for rapid metric data retrieval. This guide helps users harness the
dbt Semantic Layer's full capabilities, from creating semantic models and defining metrics to testing,
querying, and running production jobs in dbt Cloud, emphasizing the need for a dbt Cloud
Team or Enterprise account for full functionality.



## MDS examples with dlt

The next frontier for data engineers lies in harnessing advancements in AI and machine
learning to automate insights generation, enhance data quality, and predict trends. As the
ecosystem expands, staying adaptable and continuously exploring new tools and methodologies
will be key to unlocking the full potential of data for actionable insights and strategic decision-making.

In conclusion, there are many MDS formats where you could use dlt, but a classic one would be something like airflow + dlt + dbt + something for access.
Here are some examples that we or the community created:

* [Web tracking on GCP at 18x cost saving over Segment](https://dlthub.com/docs/blog/dlt-segment-migration)
* [Motherduck](https://dlthub.com/docs/blog/dlt-motherduck-demo),
* [google cloud functions ](https://docs.getdbt.com/blog/serverless-dlt-dbt-stack)
* [Gcp streaming cheaper version](https://dlthub.com/docs/blog/streaming-pubsub-json-gcp)
* [Aws streaming](https://dlthub.com/docs/blog/dlt-aws-taktile-blog)

### Want to discuss?

[Join our slack community](https://dlthub.com/community) to take part in the conversation.